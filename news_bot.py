import os
import requests
import feedparser
import time
from datetime import datetime
import urllib.parse

WEBHOOK_URL = os.getenv("FEISHU_WEBHOOK")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

RSS_FEEDS = [
    "https://news.google.com/rss/search?q=southeast+asia+ecommerce+OR+cross-border+OR+logistics+OR+policy+OR+weather&hl=en&gl=SG&ceid=SG:en",
    "https://news.google.com/rss/search?q=philippines+ecommerce+OR+cross-border+OR+logistics+OR+policy+OR+weather&hl=en&gl=SG&ceid=SG:en",
    "https://news.google.com/rss/search?q=thailand+ecommerce+OR+cross-border+OR+logistics+OR+policy+OR+weather&hl=en&gl=SG&ceid=SG:en",
    "https://news.google.com/rss/search?q=malaysia+ecommerce+OR+cross-border+OR+logistics+OR+policy+OR+weather&hl=en&gl=SG&ceid=SG:en",
    "https://news.google.com/rss/search?q=vietnam+ecommerce+OR+cross-border+OR+logistics+OR+policy+OR+weather&hl=en&gl=SG&ceid=SG:en"
]

def get_latest_news():
    news_items = []
    for url in RSS_FEEDS:
        try:
            feed = feedparser.parse(url)
            for entry in feed.entries[:2]:  # æ¯ä¸ªæºå–å‰2æ¡
                summary_text = entry.get("summary", "")  # RSS æè¿°å­—æ®µ
                news_items.append({
                    "title": entry.title,
                    "link": entry.link,
                    "rss_summary": summary_text
                })
        except Exception as e:
            print("RSSæŠ“å–å¤±è´¥:", e)
    return news_items

def summarize_with_gpt(news_item, retries=3, delay=2):
    """è°ƒç”¨ OpenAI GPT ç”Ÿæˆä¸­æ–‡æ‘˜è¦"""
    prompt_text = f"æ–°é—»æ ‡é¢˜ï¼š{news_item['title']}\næ–°é—»æ‘˜è¦ï¼š{news_item['rss_summary']}\nè¯·ç”¨ä¸­æ–‡å†™ä¸€å¥15å­—å†…æ‘˜è¦ï¼Œçªå‡ºç»æµã€æ”¿ç­–æˆ–å¤©æ°”å¯¹ç”µå•†å½±å“ã€‚"
    for attempt in range(retries):
        try:
            response = requests.post(
                "https://api.openai.com/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {OPENAI_API_KEY}",
                    "Content-Type": "application/json",
                },
                json={
                    "model": "gpt-3.5-turbo",
                    "messages": [
                        {"role": "system", "content": "ä½ æ˜¯ä¸­æ–‡è·¨å¢ƒç”µå•†æ–°é—»ç¼–è¾‘ã€‚"},
                        {"role": "user", "content": prompt_text}
                    ],
                    "max_tokens": 60,
                },
                timeout=15
            )
            response.raise_for_status()
            data = response.json()
            summary = data["choices"][0]["message"]["content"].strip()
            if summary:
                return summary
        except Exception as e:
            print(f"æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}, é‡è¯• {attempt+1}/{retries}")
            time.sleep(delay)
    # fallbackï¼šç®€å•ç¿»è¯‘æ ‡é¢˜
    return news_item['title']

def shorten_link(url):
    parsed = urllib.parse.urlparse(url)
    return f"{parsed.netloc}{parsed.path}"

def send_to_feishu(news_list):
    if not news_list:
        text = "ä»Šæ—¥æš‚æ— ç›¸å…³æ–°é—»"
    else:
        lines = []
        for news in news_list:
            lines.append(f"ğŸ“° {news['title']}\nğŸ’¬ {news['summary']}\nğŸ”— {shorten_link(news['link'])}")
        text = "\n\n".join(lines)

    payload = {
        "msg_type": "text",
        "content": {"text": f"ğŸŒ ä»Šæ—¥ä¸œå—äºšè·¨å¢ƒç”µå•†å¿«è®¯ï¼ˆ{datetime.now().strftime('%Y-%m-%d %H:%M')}ï¼‰\n\n{text}"}
    }
    try:
        r = requests.post(WEBHOOK_URL, json=payload, timeout=10)
        r.raise_for_status()
        print("é£ä¹¦å‘é€æˆåŠŸ")
    except Exception as e:
        print("é£ä¹¦å‘é€å¤±è´¥:", e)

if __name__ == "__main__":
    news_data = get_latest_news()
    for item in news_data:
        item["summary"] = summarize_with_gpt(item)
    send_to_feishu(news_data)
